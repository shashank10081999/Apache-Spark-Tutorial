{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Logistic_regression\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|\n",
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|\n",
      "|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|\n",
      "|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|\n",
      "|   Phillip White|42.0|       8010.76|              0| 6.71|     10.0|2014-04-22 12:43:12|13120 Daniel Moun...|           Smith Inc|    1|\n",
      "|  Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|2016-01-19 15:31:15|765 Tricia Row Ka...|          Love-Jones|    1|\n",
      "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cmath import inf\n",
    "df = spark.read.csv(\"customer_churn.csv\" , inferSchema= True  , header = True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Names='Cameron Williams', Age=42.0, Total_Purchase=11066.8, Account_Manager=0, Years=7.22, Num_Sites=8.0, Onboard_date=datetime.datetime(2013, 8, 30, 7, 0, 40), Location='10265 Elizabeth Mission Barkerburgh, AK 89518', Company='Harvey LLC', Churn=1),\n",
       " Row(Names='Kevin Mueller', Age=41.0, Total_Purchase=11916.22, Account_Manager=0, Years=6.5, Num_Sites=11.0, Onboard_date=datetime.datetime(2013, 8, 13, 0, 38, 46), Location='6157 Frank Gardens Suite 019 Carloshaven, RI 17756', Company='Wilson PLC', Churn=1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|       Onboard_date|\n",
      "+-------------------+\n",
      "|2013-08-30 07:00:40|\n",
      "|2013-08-13 00:38:46|\n",
      "|2016-06-29 06:20:07|\n",
      "|2014-04-22 12:43:12|\n",
      "|2016-01-19 15:31:15|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year\n",
    "df.select(\"Onboard_date\").show(5)\n",
    "df = df.withColumn(\"year\" , year(df[\"Onboard_date\"]))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"Age\", \"Total_Purchase\" , \"Account_Manager\" , \"Years\" , \"Num_Sites\" , \"year\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(df)\n",
    "final_data = output.select([\"features\" , \"Churn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|Churn|\n",
      "+--------------------+-----+\n",
      "|[42.0,11066.8,0.0...|    1|\n",
      "|[41.0,11916.22,0....|    1|\n",
      "|[38.0,12884.75,0....|    1|\n",
      "|[42.0,8010.76,0.0...|    1|\n",
      "|[37.0,9191.58,0.0...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data , test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr_churn = LogisticRegression(labelCol=\"Churn\")\n",
    "fitted_model = lr_churn.fit(train_data)\n",
    "train_summary = fitted_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|Churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[22.0,11254.38,1....|  0.0|[4.41756458663307...|[0.98808021885686...|       0.0|\n",
      "|[25.0,9672.03,0.0...|  0.0|[4.53500179316892...|[0.98938695693285...|       0.0|\n",
      "|[26.0,8787.39,1.0...|  1.0|[0.46272924411881...|[0.61366142866390...|       0.0|\n",
      "|[27.0,8628.8,1.0,...|  0.0|[5.40178112943224...|[0.99551169224334...|       0.0|\n",
      "|[28.0,8670.98,0.0...|  0.0|[7.65214072076072...|[0.99952519940122...|       0.0|\n",
      "|[28.0,9090.43,1.0...|  0.0|[1.39775890713256...|[0.80182802011358...|       0.0|\n",
      "|[28.0,11128.95,1....|  0.0|[3.97154418880577...|[0.98150422859599...|       0.0|\n",
      "|[28.0,11245.38,0....|  0.0|[3.84818869654446...|[0.97912666883077...|       0.0|\n",
      "|[29.0,5900.78,1.0...|  0.0|[3.90354503236861...|[0.98022851611409...|       0.0|\n",
      "|[29.0,9378.24,0.0...|  0.0|[4.80212888721571...|[0.99185464616346...|       0.0|\n",
      "|[29.0,9617.59,0.0...|  0.0|[4.46063803092891...|[0.98857700409130...|       0.0|\n",
      "|[29.0,10203.18,1....|  0.0|[3.69208849168811...|[0.97568599986913...|       0.0|\n",
      "|[29.0,12711.15,0....|  0.0|[5.36467634036828...|[0.99534281645191...|       0.0|\n",
      "|[29.0,13240.01,1....|  0.0|[6.59794386014650...|[0.99863868767535...|       0.0|\n",
      "|[29.0,13255.05,1....|  0.0|[4.06861119301829...|[0.98318640909161...|       0.0|\n",
      "|[30.0,6744.87,0.0...|  0.0|[3.52743129375124...|[0.97145827573786...|       0.0|\n",
      "|[30.0,7960.64,1.0...|  1.0|[2.96289011885027...|[0.95086918760012...|       0.0|\n",
      "|[30.0,8677.28,1.0...|  0.0|[4.00603923755024...|[0.98212014938164...|       0.0|\n",
      "|[30.0,10183.98,1....|  0.0|[2.68161576610245...|[0.93593307700079...|       0.0|\n",
      "|[30.0,10744.14,1....|  1.0|[1.58085014229388...|[0.82932488516295...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_summary.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|Churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[26.0,8939.61,0.0...|    0|[6.25394708180209...|[0.99808084076058...|       0.0|\n",
      "|[28.0,11204.23,0....|    0|[1.70580825620619...|[0.84629180502339...|       0.0|\n",
      "|[29.0,8688.17,1.0...|    1|[2.62551677299234...|[0.93248585008758...|       0.0|\n",
      "|[29.0,11274.46,1....|    0|[4.36661604309185...|[0.98746499655391...|       0.0|\n",
      "|[30.0,8403.78,1.0...|    0|[5.68017787055180...|[0.99659865729124...|       0.0|\n",
      "|[30.0,8874.83,0.0...|    0|[3.05572556842646...|[0.95502907299097...|       0.0|\n",
      "|[30.0,12788.37,0....|    0|[2.35588056712384...|[0.91340051272303...|       0.0|\n",
      "|[31.0,8688.21,0.0...|    0|[6.50725183297714...|[0.99850964852284...|       0.0|\n",
      "|[31.0,9574.89,0.0...|    0|[3.31734164154108...|[0.96501896314450...|       0.0|\n",
      "|[31.0,12264.68,1....|    0|[3.45755586121356...|[0.96945567542443...|       0.0|\n",
      "|[32.0,5756.12,0.0...|    0|[4.25725972305108...|[0.98603668082248...|       0.0|\n",
      "|[32.0,6367.22,1.0...|    0|[2.85398993113127...|[0.94552456131826...|       0.0|\n",
      "|[32.0,7896.65,0.0...|    0|[3.57334076586367...|[0.97270402960536...|       0.0|\n",
      "|[32.0,8575.71,0.0...|    0|[3.64504206151650...|[0.97454459200685...|       0.0|\n",
      "|[32.0,9885.12,1.0...|    1|[1.60593841580333...|[0.83284672500314...|       0.0|\n",
      "|[32.0,12479.72,0....|    0|[4.63269955640113...|[0.99036527000348...|       0.0|\n",
      "|[33.0,4711.89,0.0...|    0|[5.90578394498253...|[0.99728375234124...|       0.0|\n",
      "|[33.0,7750.54,1.0...|    0|[4.21725344040034...|[0.98547501409502...|       0.0|\n",
      "|[33.0,10309.71,1....|    0|[6.19347046649077...|[0.99796143899185...|       0.0|\n",
      "|[33.0,11370.28,1....|    0|[6.53619841214753...|[0.99855210912760...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator as BCE\n",
    "pred_and_lables = fitted_model.evaluate(test_data)\n",
    "pred_and_lables.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7637630662020906"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_eval = BCE(rawPredictionCol=\"prediction\" , labelCol=\"Churn\")\n",
    "auc = churn_eval.evaluate(pred_and_lables.predictions)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from types import new_class\n",
    "\n",
    "\n",
    "final_model = lr_churn.fit(final_data)\n",
    "new_customer = spark.read.csv(\"new_customers.csv\" , inferSchema=True , header=True)\n",
    "new_customer = new_customer.withColumn(\"year\" , year(new_customer[\"Onboard_date\"]))\n",
    "unlabeled_data = assembler.transform(new_customer).select(\"features\")\n",
    "unlabeled_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|[37.0,9935.53,1.0...|[2.22522664409239...|[0.90249210948189...|       0.0|\n",
      "|[23.0,7526.94,1.0...|[-6.2031132274102...|[0.00201903762162...|       1.0|\n",
      "|[65.0,100.0,1.0,1...|[-3.8110402065658...|[0.02164622620371...|       1.0|\n",
      "|[32.0,6487.5,0.0,...|[-5.0580350514018...|[0.00631787118868...|       1.0|\n",
      "|[32.0,13147.71,1....|[1.12051197718180...|[0.75408367060957...|       0.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_result = final_model.transform(unlabeled_data)\n",
    "final_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
